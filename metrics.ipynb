{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyter_server.transutils import base_dir\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, balanced_accuracy_score, accuracy_score, precision_score, recall_score, f1_score , precision_recall_curve\n",
    "\n",
    "import utils\n",
    "from cos_baselines import embedding_model_thresholds\n",
    "\n",
    "heuristic_thresholds = {'movies':0.8, 'books':1, 'world':None}\n",
    "\n",
    "def movies_answer_heuristic(predicted_answer, gt_answer, threshold=0.8):\n",
    "    predicted_cast = utils.spacy_extract_entities(predicted_answer)\n",
    "    intersection, union = utils.calculate_intersection_and_union(gt_answer['movie_cast'], predicted_cast)\n",
    "\n",
    "    answer_simple_heuristic = len(intersection) / len(predicted_cast) > threshold if len(predicted_cast) != 0 else True\n",
    "    return answer_simple_heuristic\n",
    "\n",
    "\n",
    "def books_answer_heuristic(predicted_answer, gt_answer, threshold=3):\n",
    "    answer_simple_heuristic = sum([1 for x in gt_answer.values() if utils.check_entity_in_sentence(x, predicted_answer)])\n",
    "\n",
    "    return answer_simple_heuristic >= threshold\n",
    "\n",
    "def world_answer_heuristic(predicted_answer, gt_answer):\n",
    "    for x in gt_answer.values():\n",
    "        for i in x:\n",
    "            if utils.check_entity_in_sentence(i, predicted_answer):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def calc_ans_heuristic(predicted_answer, gt_answer, dataset_name, heuristic_threshold):\n",
    "    if dataset_name == 'books':\n",
    "        gt = [books_answer_heuristic(x, y, heuristic_threshold) for x, y in zip(predicted_answer, gt_answer)]\n",
    "    elif dataset_name == 'movies':\n",
    "        gt = [movies_answer_heuristic(x, y, heuristic_threshold) for x, y in zip(predicted_answer, gt_answer)]\n",
    "    elif dataset_name == 'world':\n",
    "        gt = [world_answer_heuristic(x, y) for x, y in zip(predicted_answer, gt_answer)]\n",
    "    else:\n",
    "        gt = []\n",
    "\n",
    "    return gt\n",
    "\n",
    "\n",
    "def auc_plot(gt, pred, title, file_name, save_path='./'):\n",
    "    pred = [x if x <= 1 else 1.0 for x in pred]\n",
    "\n",
    "    pred = 1 - np.array(pred)\n",
    "    gt = np.array(gt)\n",
    "    gt = 1 - gt\n",
    "\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, _ = roc_curve(gt, pred)\n",
    "\n",
    "    ns_probs = [0 for _ in range(len(gt))]\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(gt, ns_probs)\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.', label='Model')\n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # add the auc score and optimal threshold to the plot\n",
    "    props = dict(boxstyle='round', facecolor='grey', alpha=0.5)\n",
    "    # plt.text(0.7, 0.02, f'AUC: {roc_auc_score(gt, pred):.3f}\\nOptimal Threshold: {optimal_threshold:.3f}\\nBalanced Acc: {bal_acc:.3f}', fontsize=8, bbox=props)\n",
    "    plt.text(0.1, 0.02, f'AUC: {roc_auc_score(gt, pred):.3f}\\n', fontsize=8, bbox=props)\n",
    "\n",
    "    # title\n",
    "    plt.title(title)\n",
    "\n",
    "    # show the plot\n",
    "    save_path = os.path.join(save_path, file_name)\n",
    "    plt.savefig(save_path)\n",
    "    plt.cla()\n",
    "\n",
    "\n",
    "def calc_auc_and_bal_acc(gt, pred):\n",
    "    #  gt = true if not hallucinated, false if hallucinated\n",
    "    #  pred is the cosine similarity between the generated question and the original question\n",
    "    #  cosine similarity is high if it is not hallucinated\n",
    "\n",
    "\n",
    "    pred = np.array([x if x <= 1 else 1.0 for x in pred])\n",
    "    gt = np.array(gt)\n",
    "\n",
    "    optimal_threshold = embedding_model_thresholds[\"sbert\"]\n",
    "    # optimal_threshold = 0.8\n",
    "    pred_not_hallucinated = pred > optimal_threshold\n",
    "\n",
    "\n",
    "    # gt = 1 - gt\n",
    "    # pred = 1 - pred\n",
    "\n",
    "\n",
    "    bal_acc = balanced_accuracy_score(1 - gt, 1 - pred_not_hallucinated)\n",
    "    auc = roc_auc_score(gt, pred)\n",
    "\n",
    "    return auc, bal_acc, 1 - gt, 1 - pred_not_hallucinated\n",
    "\n",
    "def calc_auc_and_acc(base_dir, dataset_name, k_range, avg_max='avg'):\n",
    "    heuristic_threshold = heuristic_thresholds[dataset_name]\n",
    "    dataset_dir = os.path.join(base_dir, dataset_name)\n",
    "    print(\"Dataset: \", dataset_name, \"##############################################\")\n",
    "\n",
    "    ans_models = [\"gpt\", \"llama_7b\"]\n",
    "\n",
    "    for ans_model in ans_models:\n",
    "        print(\"Answer Model: \", ans_model, \"####################\")\n",
    "\n",
    "        file_path = os.path.join(dataset_dir, f'{ans_model}_combined.pkl')\n",
    "        with open(file_path, 'rb') as handle:\n",
    "            results = pkl.load(handle)\n",
    "\n",
    "            gt_answers = [res['answer_args'] for res in results]\n",
    "            pred_ans = [res['predicted_answer'] for res in results]\n",
    "\n",
    "\n",
    "            gt = calc_ans_heuristic(pred_ans, gt_answers, dataset_name, heuristic_threshold)\n",
    "\n",
    "\n",
    "            # exp_type = 'predicted_questions_const'\n",
    "            exp_type = 'predicted_questions_var'\n",
    "\n",
    "            print(\"Experiment Type: \", exp_type)\n",
    "\n",
    "            predicted_questions_cosine = [{key: value for m_res in res[exp_type] for key, value in m_res.items()} for res in results]\n",
    "\n",
    "            pred_questions_cosine_gpt = [res['gpt'][:k_range] for res in predicted_questions_cosine]\n",
    "            pred_questions_cosine_gpt = [[item[2] for item in inner_list] for inner_list in pred_questions_cosine_gpt]\n",
    "\n",
    "            pred_questions_cosine_l7 = [res['l7'][:k_range] for res in predicted_questions_cosine]\n",
    "            pred_questions_cosine_l7 = [[item[2] for item in inner_list] for inner_list in pred_questions_cosine_l7]\n",
    "\n",
    "            pred_questions_cosine_l13 = [res['l13'][:k_range] for res in predicted_questions_cosine]\n",
    "            pred_questions_cosine_l13 = [[item[2] for item in inner_list] for inner_list in pred_questions_cosine_l13]\n",
    "\n",
    "            pred_questions_cosine_ensemble = [res1 + res2 + res3 for res1, res2, res3 in zip(pred_questions_cosine_gpt, pred_questions_cosine_l7, pred_questions_cosine_l13)]\n",
    "\n",
    "            for f, f_name in zip([np.max, np.average], ['max', 'avg']):\n",
    "                print(f_name + '\\n')\n",
    "                pred_questions_cosine_gpt_ = [f(x) for x in pred_questions_cosine_gpt]\n",
    "                pred_questions_cosine_l7_ = [f(x) for x in pred_questions_cosine_l7]\n",
    "                pred_questions_cosine_l13_ = [f(x) for x in pred_questions_cosine_l13]\n",
    "                pred_questions_cosine_ensemble_ = [f(x) for x in pred_questions_cosine_ensemble]\n",
    "\n",
    "\n",
    "                print(f'k={k_range}, heuristic_threshold={heuristic_threshold}')\n",
    "                print(f'Hallucination rate: {1 - (sum(gt)/len(gt)):.3f}')\n",
    "                gpt_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_gpt_)\n",
    "                print(f'gpt:\\n  AUC: {gpt_res[0]:.3f}, Balanced Acc: {gpt_res[1]:.3f}')\n",
    "                l7_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_l7_)\n",
    "                print(f'llama7:\\n  AUC: {l7_res[0]:.3f}, Balanced Acc: {l7_res[1]:.3f}')\n",
    "                l13_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_l13_)\n",
    "                print(f'llama13:\\n  AUC: {l13_res[0]:.3f}, Balanced Acc: {l13_res[1]:.3f}')\n",
    "                ensemble_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_ensemble_)\n",
    "                print(f'ensemble:\\n  AUC: {ensemble_res[0]:.3f}, Balanced Acc: {ensemble_res[1]:.3f}')\n",
    "                print('\\n\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "BASE_DIR = \"/Users/jiviteshjain/Documents/CMU/Coursework/Sem-1/ANLP/Assignment-3\"",
   "id": "45fdc9c9b9dca910",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_name = \"books\"\n",
    "base_dir = BASE_DIR\n",
    "k_range = 5\n",
    "\n",
    "\n",
    "heuristic_threshold = heuristic_thresholds[dataset_name]\n",
    "dataset_dir = os.path.join(base_dir, dataset_name)\n",
    "print(\"Dataset: \", dataset_name, \"##############################################\")\n",
    "\n",
    "ans_model = \"gpt\"\n",
    "print(\"Answer Model: \", ans_model, \"####################\")\n",
    "\n",
    "file_path = os.path.join(dataset_dir, f'{ans_model}_combined.pkl')\n",
    "with open(file_path, 'rb') as handle:\n",
    "    results = pkl.load(handle)\n",
    "\n",
    "    gt_answers = [res['answer_args'] for res in results]\n",
    "    pred_ans = [res['predicted_answer'] for res in results]\n",
    "\n",
    "\n",
    "    # True in gt means the answer is correct (matches ground truth ans) and not hallucinated.\n",
    "    gt = calc_ans_heuristic(pred_ans, gt_answers, dataset_name, heuristic_threshold)\n",
    "\n",
    "\n",
    "    # exp_type = 'predicted_questions_const'\n",
    "    exp_type = 'predicted_questions_var'\n",
    "\n",
    "    print(\"Experiment Type: \", exp_type)\n",
    "\n",
    "    predicted_questions_cosine = [{key: value for m_res in res[exp_type] for key, value in m_res.items()} for res in results]\n",
    "\n",
    "    pred_questions_cosine_gpt = [res['gpt'][:k_range] for res in predicted_questions_cosine]\n",
    "    pred_questions_cosine_gpt = [[item[2] for item in inner_list] for inner_list in pred_questions_cosine_gpt]\n",
    "\n",
    "    pred_questions_cosine_l7 = [res['l7'][:k_range] for res in predicted_questions_cosine]\n",
    "    pred_questions_cosine_l7 = [[item[2] for item in inner_list] for inner_list in pred_questions_cosine_l7]\n",
    "\n",
    "    pred_questions_cosine_l13 = [res['l13'][:k_range] for res in predicted_questions_cosine]\n",
    "    pred_questions_cosine_l13 = [[item[2] for item in inner_list] for inner_list in pred_questions_cosine_l13]\n",
    "\n",
    "    pred_questions_cosine_ensemble = [res1 + res2 + res3 for res1, res2, res3 in zip(pred_questions_cosine_gpt, pred_questions_cosine_l7, pred_questions_cosine_l13)]\n",
    "\n",
    "    for f, f_name in zip([np.max, np.average], ['max', 'avg']):\n",
    "        print(f_name + '\\n')\n",
    "        pred_questions_cosine_gpt_ = [f(x) for x in pred_questions_cosine_gpt]\n",
    "        pred_questions_cosine_l7_ = [f(x) for x in pred_questions_cosine_l7]\n",
    "        pred_questions_cosine_l13_ = [f(x) for x in pred_questions_cosine_l13]\n",
    "        pred_questions_cosine_ensemble_ = [f(x) for x in pred_questions_cosine_ensemble]\n",
    "\n",
    "        # if pred is high, it means the gen question matched the original question,\n",
    "        # so answer was not hallucinated.\n",
    "\n",
    "        print(f'k={k_range}, heuristic_threshold={heuristic_threshold}')\n",
    "        print(f'Hallucination rate: {1 - (sum(gt)/len(gt)):.3f}')\n",
    "        gpt_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_gpt_)\n",
    "        print(f'gpt:\\n  AUC: {gpt_res[0]:.3f}, Balanced Acc: {gpt_res[1]:.3f}')\n",
    "        l7_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_l7_)\n",
    "        print(f'llama7:\\n  AUC: {l7_res[0]:.3f}, Balanced Acc: {l7_res[1]:.3f}')\n",
    "        l13_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_l13_)\n",
    "        print(f'llama13:\\n  AUC: {l13_res[0]:.3f}, Balanced Acc: {l13_res[1]:.3f}')\n",
    "        ensemble_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_ensemble_)\n",
    "        print(f'ensemble:\\n  AUC: {ensemble_res[0]:.3f}, Balanced Acc: {ensemble_res[1]:.3f}')\n",
    "        print('\\n\\n')\n",
    "\n",
    "        gt_hallucinated = gpt_res[2]\n",
    "        pred_hallucinated = gpt_res[3]"
   ],
   "id": "ffce374106f62562",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=gt_hallucinated, y_pred=pred_hallucinated, labels=[1, 0])\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "            yticklabels=[\"Actual Positive\", \"Actual Negative\"])\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Ground Truth\")\n",
    "plt.title(\"Books Confusion Matrix\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig(\"books_confusion_matrix.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ],
   "id": "584d34f870fb3498",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tp = conf_matrix[0, 0]\n",
    "fp = conf_matrix[1, 0]\n",
    "tn = conf_matrix[1, 1]\n",
    "fn = conf_matrix[0, 1]\n",
    "\n",
    "print(tp, fn)\n",
    "print(fp, tn)\n",
    "print(conf_matrix)"
   ],
   "id": "9453e53c422427c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fpr = fp / (fp + tn)\n",
    "fnr = fn / (fn + tp)\n",
    "\n",
    "ppv = tp / (tp + fp)\n",
    "npv = tn / (tn + fn)"
   ],
   "id": "e1023d5a2b334d34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"PPV: \", ppv)\n",
    "print(\"NPV: \", npv)\n",
    "print(\"FPR: \", fpr)\n",
    "print(\"FNR: \", fnr)"
   ],
   "id": "7bba4a9eb2b77091",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ppv and npv: higher is better\n",
    "# fpr and fnr: lower is better\n",
    "\n",
    "# ppv and fpr are bad\n",
    "# ppv is low -> what is classifies as hallucination is not hallucination\n",
    "# fpr is high -> not hallucination classified as hallucination\n",
    "\n",
    "# too many false positives"
   ],
   "id": "a744ac4ca7c823c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# find false positive indices\n",
    "gt_is_false_indices = gt_hallucinated == 0\n",
    "fp_indices = np.where(pred_hallucinated[gt_is_false_indices] == 1)[0]\n",
    "fp_indices"
   ],
   "id": "677502ae5a681102",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fp_examples = [results[i] for i in fp_indices]",
   "id": "5f356a31ceddc671",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for k in range(5):\n",
    "    print(\"########################\")\n",
    "    print(\"k = \", k+1)\n",
    "    print(\"########################\")\n",
    "    calc_auc_and_acc(BASE_DIR, \"books\", k+1, avg_max='avg')\n",
    "\n"
   ],
   "id": "fb6e84160d3815fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_calc(base_dir, dataset_name, k_range):\n",
    "    heuristic_threshold = heuristic_thresholds[dataset_name]\n",
    "    dataset_dir = os.path.join(base_dir, dataset_name)\n",
    "    print(\"Dataset: \", dataset_name, \"##############################################\")\n",
    "\n",
    "    ans_models = [\"gpt\"]\n",
    "\n",
    "    for ans_model in ans_models:\n",
    "        print(\"Answer Model: \", ans_model, \"####################\")\n",
    "\n",
    "        file_path = os.path.join(dataset_dir, f'{ans_model}_combined.pkl')\n",
    "        with open(file_path, 'rb') as handle:\n",
    "            results = pkl.load(handle)\n",
    "\n",
    "            gt_answers = [res['answer_args'] for res in results]\n",
    "            pred_ans = [res['predicted_answer'] for res in results]\n",
    "\n",
    "\n",
    "            gt = calc_ans_heuristic(pred_ans, gt_answers, dataset_name, heuristic_threshold)\n",
    "\n",
    "\n",
    "            # exp_type = 'predicted_questions_const'\n",
    "            exp_type = 'predicted_questions_var'\n",
    "\n",
    "            print(\"Experiment Type: \", exp_type)\n",
    "\n",
    "            predicted_questions_cosine = [{key: value for m_res in res[exp_type] for key, value in m_res.items()} for res in results]\n",
    "\n",
    "            pred_questions_cosine_gpt = [res['gpt'][:k_range] for res in predicted_questions_cosine]\n",
    "            pred_questions_cosine_gpt = [[item[2] for item in inner_list] for inner_list in pred_questions_cosine_gpt]\n",
    "\n",
    "            pred_questions_cosine_l7 = [res['l7'][:k_range] for res in predicted_questions_cosine]\n",
    "            pred_questions_cosine_l7 = [[item[2] for item in inner_list] for inner_list in pred_questions_cosine_l7]\n",
    "\n",
    "            pred_questions_cosine_l13 = [res['l13'][:k_range] for res in predicted_questions_cosine]\n",
    "            pred_questions_cosine_l13 = [[item[2] for item in inner_list] for inner_list in pred_questions_cosine_l13]\n",
    "\n",
    "            pred_questions_cosine_ensemble = [res1 + res2 + res3 for res1, res2, res3 in zip(pred_questions_cosine_gpt, pred_questions_cosine_l7, pred_questions_cosine_l13)]\n",
    "\n",
    "            for f, f_name in zip([np.max], ['max']):\n",
    "                print(f_name + '\\n')\n",
    "                pred_questions_cosine_gpt_ = [f(x) for x in pred_questions_cosine_gpt]\n",
    "                pred_questions_cosine_l7_ = [f(x) for x in pred_questions_cosine_l7]\n",
    "                pred_questions_cosine_l13_ = [f(x) for x in pred_questions_cosine_l13]\n",
    "                pred_questions_cosine_ensemble_ = [f(x) for x in pred_questions_cosine_ensemble]\n",
    "\n",
    "\n",
    "                print(f'k={k_range}, heuristic_threshold={heuristic_threshold}')\n",
    "                print(f'Hallucination rate: {1 - (sum(gt)/len(gt)):.3f}')\n",
    "                gpt_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_gpt_)\n",
    "                print(f'gpt:\\n  AUC: {gpt_res[0]:.3f}, Balanced Acc: {gpt_res[1]:.3f}')\n",
    "                # l7_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_l7_)\n",
    "                # print(f'llama7:\\n  AUC: {l7_res[0]:.3f}, Balanced Acc: {l7_res[1]:.3f}')\n",
    "                # l13_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_l13_)\n",
    "                # print(f'llama13:\\n  AUC: {l13_res[0]:.3f}, Balanced Acc: {l13_res[1]:.3f}')\n",
    "                # ensemble_res = calc_auc_and_bal_acc(gt, pred_questions_cosine_ensemble_)\n",
    "                # print(f'ensemble:\\n  AUC: {ensemble_res[0]:.3f}, Balanced Acc: {ensemble_res[1]:.3f}')\n",
    "                # print('\\n\\n')\n",
    "                return gpt_res[0], gpt_res[1]\n",
    "\n",
    "aucs = []\n",
    "baccs = []\n",
    "for k in range(5):\n",
    "    auc, bacc = plot_calc(BASE_DIR, \"books\", k+1)\n",
    "    aucs.append(auc)\n",
    "    baccs.append(bacc)"
   ],
   "id": "18abbd506ce114e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "x_vals = np.arange(1, 6)\n",
    "# Plot the two series\n",
    "ax.plot(x_vals, aucs, label=\"AUC\", marker=\"o\")\n",
    "ax.plot(x_vals, baccs, label=\"B-Accuracy\", marker=\"s\")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel(\"K Size\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Books Dataset\")\n",
    "ax.legend()\n",
    "\n",
    "# Show grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "fig.savefig(\"books_auc_bacc.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "id": "c95728c373dc7d1c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
